{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999175fc-0b8c-4eed-bada-9b8e33bead3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3374803015.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 18\u001b[1;36m\u001b[0m\n\u001b[1;33m    from ./data_provider.data_factory import data_provider\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 导入用户的模块\n",
    "from data_provider.data_factory import data_provider\n",
    "from exp.exp_basic import Exp_Basic\n",
    "from models import SparseTSF\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate, visual, test_params_flop\n",
    "from utils.metrics import metric\n",
    "\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5206a327-1d3c-41ee-a5c1-1002c96feba1",
   "metadata": {},
   "source": [
    "数据集类简化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df55e12e-aaeb-4917-93f6-807ffabebae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleETTDataset(Dataset):\n",
    "    def __init__(self, data, seq_len, pred_len):\n",
    "        \"\"\"\n",
    "        data: numpy array, shape [time, features]\n",
    "        seq_len: 输入序列长度\n",
    "        pred_len: 预测序列长度\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - self.pred_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index : index + self.seq_len, :]\n",
    "        y = self.data[index + self.seq_len : index + self.seq_len + self.pred_len, :]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e194227-c914-499c-a17f-2578f6967ceb",
   "metadata": {},
   "source": [
    "模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2340eda3-c0ff-4ae7-9770-5c98d28d5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # get parameters\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.enc_in = configs.enc_in\n",
    "        self.period_len = configs.period_len\n",
    "\n",
    "        self.seg_num_x = self.seq_len // self.period_len\n",
    "        self.seg_num_y = self.pred_len // self.period_len\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=1 + 2 * (self.period_len // 2),\n",
    "                                stride=1, padding=self.period_len // 2, padding_mode=\"zeros\", bias=False)\n",
    "\n",
    "        self.linear = nn.Linear(self.seg_num_x, self.seg_num_y, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # normalization and permute     b,s,c -> b,c,s\n",
    "        seq_mean = torch.mean(x, dim=1).unsqueeze(1)\n",
    "        x = (x - seq_mean).permute(0, 2, 1)\n",
    "\n",
    "        # 1D convolution aggregation\n",
    "        x = self.conv1d(x.reshape(-1, 1, self.seq_len)).reshape(-1, self.enc_in, self.seq_len) + x\n",
    "\n",
    "        # downsampling: b,c,s -> bc,n,w -> bc,w,n\n",
    "        x = x.reshape(-1, self.seg_num_x, self.period_len).permute(0, 2, 1)\n",
    "\n",
    "        # sparse forecasting\n",
    "        y = self.linear(x)  # bc,w,m\n",
    "\n",
    "        # upsampling: bc,w,m -> bc,m,w -> b,c,s\n",
    "        y = y.permute(0, 2, 1).reshape(batch_size, self.enc_in, self.pred_len)\n",
    "\n",
    "        # permute and denorm\n",
    "        y = y.permute(0, 2, 1) + seq_mean\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1b275d0-ad39-4282-8959-443a0a2e3a34",
   "metadata": {},
   "source": [
    "训练函数和测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36603800-a43f-4c43-915f-1a02ddb6ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, config, device):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=config.learning_rate, \n",
    "        steps_per_epoch=len(train_loader), epochs=config.train_epochs\n",
    "    )\n",
    "    best_val_loss = float('inf')\n",
    "    patience = config.patience\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(config.train_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                output_val = model(x_val)\n",
    "                loss_val = criterion(output_val, y_val)\n",
    "                val_loss += loss_val.item() * x_val.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{config.train_epochs}]: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        # 保存最优模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), config.best_model_path)\n",
    "            print(\"Best model saved.\")\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def test_model(model, test_loader, device, best_model_path):\n",
    "    # 加载最优模型\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    test_loss = 0.0\n",
    "    mae = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "            output = model(x_test)\n",
    "            loss = criterion(output, y_test)\n",
    "            test_loss += loss.item() * x_test.size(0)\n",
    "            mae += torch.mean(torch.abs(output - y_test)).item() * x_test.size(0)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    mae /= len(test_loader.dataset)\n",
    "    print(f\"Test Loss (MSE): {test_loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "    return test_loss, mae\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4da58c55-e374-4d90-94d1-25c596ca2ac8",
   "metadata": {},
   "source": [
    "数据加载和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd536358-fe53-4912-8802-12e06edce1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    从CSV文件中加载数据，假设第一列是日期，可选。\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    if 'date' in df.columns:\n",
    "        df = df.drop(columns=['date'])\n",
    "    data = df.values  # [time, features]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed6cdfcf-f7de-4fb5-819b-d725cc23e5bc",
   "metadata": {},
   "source": [
    "训练和检测流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dda98c36-4976-4759-acb8-e0408c3659bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义路径和参数\n",
    "root_path_name = './'\n",
    "data_path_name_train = 'ETTh2.csv'  # 用于训练的ETTh1数据集\n",
    "data_path_name_test = 'ETTh1.csv'   # 用于测试的ETTh2数据集\n",
    "\n",
    "# 检查并创建日志目录\n",
    "log_dir = './logs'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "model_name = 'SparseTSF'\n",
    "\n",
    "seq_len = 720\n",
    "pred_len_list = [96, 192, 336, 720]\n",
    "\n",
    "# 加载训练和测试数据\n",
    "data_ETTh1 = load_data(os.path.join(root_path_name, data_path_name_train))\n",
    "data_ETTh2 = load_data(os.path.join(root_path_name, data_path_name_test))\n",
    "\n",
    "# 标准化：使用训练集的均值和标准差\n",
    "mean = data_ETTh1.mean(axis=0)\n",
    "std = data_ETTh1.std(axis=0)\n",
    "data_ETTh1 = (data_ETTh1 - mean) / std\n",
    "data_ETTh2 = (data_ETTh2 - mean) / std  # 使用相同的均值和标准差\n",
    "\n",
    "# 划分训练集和验证集（在ETTh1上）\n",
    "n = len(data_ETTh1)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.1\n",
    "train_end = int(n * train_ratio)\n",
    "val_end = int(n * (train_ratio + val_ratio))\n",
    "\n",
    "train_data = data_ETTh1[:train_end]\n",
    "val_data = data_ETTh1[train_end:val_end]\n",
    "\n",
    "# 创建Dataset和DataLoader\n",
    "batch_size = 256\n",
    "\n",
    "def create_dataloaders(train_data, val_data, test_data, seq_len, pred_len, batch_size):\n",
    "    train_dataset = SimpleETTDataset(train_data, seq_len, pred_len)\n",
    "    val_dataset = SimpleETTDataset(val_data, seq_len, pred_len)\n",
    "    test_dataset = SimpleETTDataset(test_data, seq_len, pred_len)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# 定义一个字典来存储每个 pred_len 的结果\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d773e43a-2731-4fe4-ade9-83d7697e2446",
   "metadata": {},
   "source": [
    "循环使用不同参数进行检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ab7dff6-d1fc-43bc-af45-9bdd4a0732fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Training with pred_len = 96 ====================\n",
      "\n",
      "Epoch [1/30]: Train Loss = 0.6746, Val Loss = 0.3296\n",
      "Best model saved.\n",
      "Epoch [2/30]: Train Loss = 0.4223, Val Loss = 0.2935\n",
      "Best model saved.\n",
      "Epoch [3/30]: Train Loss = 0.3675, Val Loss = 0.2560\n",
      "Best model saved.\n",
      "Epoch [4/30]: Train Loss = 0.3360, Val Loss = 0.2433\n",
      "Best model saved.\n",
      "Epoch [5/30]: Train Loss = 0.3163, Val Loss = 0.2322\n",
      "Best model saved.\n",
      "Epoch [6/30]: Train Loss = 0.3085, Val Loss = 0.2310\n",
      "Best model saved.\n",
      "Epoch [7/30]: Train Loss = 0.3077, Val Loss = 0.2290\n",
      "Best model saved.\n",
      "Epoch [8/30]: Train Loss = 0.3078, Val Loss = 0.2270\n",
      "Best model saved.\n",
      "Epoch [9/30]: Train Loss = 0.3080, Val Loss = 0.2352\n",
      "Epoch [10/30]: Train Loss = 0.3072, Val Loss = 0.2304\n",
      "Epoch [11/30]: Train Loss = 0.3068, Val Loss = 0.2226\n",
      "Best model saved.\n",
      "Epoch [12/30]: Train Loss = 0.3067, Val Loss = 0.2231\n",
      "Epoch [13/30]: Train Loss = 0.3063, Val Loss = 0.2334\n",
      "Epoch [14/30]: Train Loss = 0.3072, Val Loss = 0.2260\n",
      "Epoch [15/30]: Train Loss = 0.3060, Val Loss = 0.2259\n",
      "Epoch [16/30]: Train Loss = 0.3067, Val Loss = 0.2249\n",
      "Early stopping triggered.\n",
      "Training completed in 57.41 seconds.\n",
      "\n",
      "Testing on ETTh2 with pred_len = 96\n",
      "Test Loss (MSE): 0.0717, Test MAE: 0.1695\n",
      "\n",
      "==================== Training with pred_len = 192 ====================\n",
      "\n",
      "Epoch [1/30]: Train Loss = 0.7328, Val Loss = 0.3747\n",
      "Best model saved.\n",
      "Epoch [2/30]: Train Loss = 0.5443, Val Loss = 0.3492\n",
      "Best model saved.\n",
      "Epoch [3/30]: Train Loss = 0.4591, Val Loss = 0.3183\n",
      "Best model saved.\n",
      "Epoch [4/30]: Train Loss = 0.4117, Val Loss = 0.3015\n",
      "Best model saved.\n",
      "Epoch [5/30]: Train Loss = 0.3979, Val Loss = 0.2985\n",
      "Best model saved.\n",
      "Epoch [6/30]: Train Loss = 0.3957, Val Loss = 0.3036\n",
      "Epoch [7/30]: Train Loss = 0.3956, Val Loss = 0.2866\n",
      "Best model saved.\n",
      "Epoch [8/30]: Train Loss = 0.3957, Val Loss = 0.3003\n",
      "Epoch [9/30]: Train Loss = 0.3942, Val Loss = 0.2948\n",
      "Epoch [10/30]: Train Loss = 0.3944, Val Loss = 0.3035\n",
      "Epoch [11/30]: Train Loss = 0.3963, Val Loss = 0.3050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m     25\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 测试模型在ETTh2上的泛化性能\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, config, device)\u001b[0m\n\u001b[0;32m     16\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, y)\n\u001b[0;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mD:\\withoutchinese_anaconda\\envs\\SparseTSF\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\withoutchinese_anaconda\\envs\\SparseTSF\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 28\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# normalization and permute     b,s,c -> b,c,s\u001b[39;00m\n\u001b[0;32m     27\u001b[0m seq_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m x \u001b[38;5;241m=\u001b[39m (\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseq_mean\u001b[49m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 1D convolution aggregation\u001b[39;00m\n\u001b[0;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d(x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_in, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len) \u001b[38;5;241m+\u001b[39m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for pred_len in pred_len_list:\n",
    "    print(f\"\\n{'='*20} Training with pred_len = {pred_len} {'='*20}\\n\")\n",
    "    \n",
    "    # 创建对应的 DataLoader\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_data, val_data, data_ETTh2, seq_len, pred_len, batch_size)\n",
    "    \n",
    "    # 配置参数\n",
    "    config = SimpleNamespace(\n",
    "        seq_len=seq_len,\n",
    "        pred_len=pred_len,\n",
    "        enc_in=data_ETTh1.shape[1],\n",
    "        period_len=24,\n",
    "        d_model=64,\n",
    "        model_type='graph',\n",
    "        learning_rate=0.03,\n",
    "        train_epochs=30,\n",
    "        patience=5,\n",
    "        best_model_path=f\"./logs/{model_name}_ETTh1_seq{seq_len}_pred{pred_len}.pth\"\n",
    "    )\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = Model(config).to(device)\n",
    "    \n",
    "    # 训练模型\n",
    "    start_time = time.time()\n",
    "    train_model(model, train_loader, val_loader, config, device)\n",
    "    print(f\"Training completed in {time.time() - start_time:.2f} seconds.\")\n",
    "    \n",
    "    # 测试模型在ETTh2上的泛化性能\n",
    "    print(f\"\\nTesting on ETTh2 with pred_len = {pred_len}\")\n",
    "    test_loss, test_mae = test_model(model, test_loader, device, config.best_model_path)\n",
    "    \n",
    "    # 存储结果\n",
    "    results[pred_len] = {\n",
    "        'Test MSE': test_loss,\n",
    "        'Test MAE': test_mae\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b71a21d6-8cc8-41ee-9a0d-d1276973a537",
   "metadata": {},
   "source": [
    "显示结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d37f829-1844-4e3a-810a-8ce7c36493fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*20 + \" Summary of Results \" + \"=\"*20)\n",
    "for pred_len in pred_len_list:\n",
    "    mse = results[pred_len]['Test MSE']\n",
    "    mae = results[pred_len]['Test MAE']\n",
    "    print(f\"pred_len = {pred_len}: Test MSE = {mse:.4f}, Test MAE = {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ce7d5-56df-4e8d-8111-de384c08d576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
